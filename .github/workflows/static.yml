# Workflow for building and deploying the docfind example to GitHub Pages
name: Build and deploy example to Pages

on:
  # Runs on pushes targeting the default branch
  push:
    branches: ["main"]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build-and-deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install docfind using install script
        run: |
          # Download and run the install script
          curl -fsSL https://microsoft.github.io/docfind/install.sh | sh
          # Add to PATH
          echo "$HOME/.local/bin" >> $GITHUB_PATH
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Download and prepare AG News dataset
        run: |
          cd static
          python3 ../scripts/prepare_demo_dataset.py 50000
          cp ../scripts/documents.json .
          echo "Dataset prepared successfully"
          ls -lh documents.json

      - name: Build search index with docfind
        run: |
          cd static
          # Build the WASM and JS files and measure time
          START_TIME=$(date +%s%3N)
          docfind documents.json .
          END_TIME=$(date +%s%3N)
          BUILD_TIME=$((END_TIME - START_TIME))
          echo "BUILD_TIME_MS=$BUILD_TIME" >> $GITHUB_ENV
          echo "Index built successfully in ${BUILD_TIME}ms"
          ls -lh docfind.js docfind_bg.wasm

      - name: Collect metrics
        run: |
          cd static
          # Count documents in the JSON file
          DOC_COUNT=$(python3 -c "import json; print(len(json.load(open('documents.json'))))")
          
          # Get dataset size
          DATASET_SIZE=$(stat -f%z documents.json 2>/dev/null || stat -c%s documents.json)
          DATASET_SIZE_MB=$(echo "scale=1; $DATASET_SIZE / 1024 / 1024" | bc)
          
          # Get WASM file size
          WASM_SIZE=$(stat -f%z docfind_bg.wasm 2>/dev/null || stat -c%s docfind_bg.wasm)
          WASM_SIZE_MB=$(echo "scale=1; $WASM_SIZE / 1024 / 1024" | bc)
          
          # Compress with brotli to measure compressed size
          brotli -c docfind_bg.wasm > docfind_bg.wasm.br
          COMPRESSED_SIZE=$(stat -f%z docfind_bg.wasm.br 2>/dev/null || stat -c%s docfind_bg.wasm.br)
          COMPRESSED_SIZE_MB=$(echo "scale=1; $COMPRESSED_SIZE / 1024 / 1024" | bc)
          
          # Calculate compression ratio
          COMPRESSION_RATIO=$(echo "scale=1; 100 - ($COMPRESSED_SIZE * 100 / $WASM_SIZE)" | bc)
          
          # Format build time (convert ms to seconds if > 1000ms)
          BUILD_TIME_DISPLAY="${BUILD_TIME_MS}ms"
          if [ "$BUILD_TIME_MS" -gt 1000 ]; then
            BUILD_TIME_SEC=$(echo "scale=2; $BUILD_TIME_MS / 1000" | bc)
            BUILD_TIME_DISPLAY="${BUILD_TIME_SEC}s"
          fi
          
          # Create metrics.json for the demo page
          cat > metrics.json << EOF
          {
            "document_count": "$DOC_COUNT",
            "wasm_size": "${WASM_SIZE_MB} MB",
            "compressed_size": "${COMPRESSED_SIZE_MB} MB",
            "compression_ratio": "${COMPRESSION_RATIO}%",
            "build_time": "$BUILD_TIME_DISPLAY"
          }
          EOF
          
          echo "=== Build Metrics ==="
          echo "Documents indexed: $DOC_COUNT"
          echo "Dataset size: ${DATASET_SIZE_MB} MB"
          echo "Index build time: $BUILD_TIME_DISPLAY"
          echo "WASM size: ${WASM_SIZE_MB} MB"
          echo "Brotli compressed: ${COMPRESSED_SIZE_MB} MB"
          echo "Compression ratio: ${COMPRESSION_RATIO}%"
          cat metrics.json

      - name: Clean up temporary files
        run: |
          cd static
          # Remove dataset files to reduce deployment size
          rm -f train.csv test.csv documents.json docfind_bg.wasm.br

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'static'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
